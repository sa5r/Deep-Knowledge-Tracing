{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e9aceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8247a82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MYDataset(Dataset):\n",
    "    def __init__(self, data):        \n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self,):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, id):\n",
    "        return self.data[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f954b0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_counter = 124\n",
    "\n",
    "def load_data(path, split = False):\n",
    "    global questions_counter\n",
    "    lines = ''\n",
    "    with open(path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Counting questions\n",
    "    # lines_counter =0\n",
    "    # while lines_counter < len(lines):\n",
    "    #     questions = lines[lines_counter+1].split(',')\n",
    "    #     for i,v in enumerate(questions):\n",
    "    #         try:\n",
    "    #             if int(v) not in questions_map.keys():\n",
    "    #                 questions_map[int(v)] = questions_counter\n",
    "    #                 questions_counter += 1\n",
    "    #         except ValueError:\n",
    "    #                 pass\n",
    "    #     lines_counter+=3\n",
    "\n",
    "    # Filling data\n",
    "    lines_counter =0\n",
    "    data = []\n",
    "    while lines_counter < len(lines):\n",
    "        questions = lines[lines_counter+1].split(',')\n",
    "        answers = lines[lines_counter+2].split(',')\n",
    "        interactions = {}\n",
    "        for i,v in enumerate(questions):\n",
    "            try:\n",
    "                question_id = int(v)\n",
    "                # +1 for next question id\n",
    "                # +1 for y\n",
    "                interaction = [0.0] * (questions_counter * 2 + 2)\n",
    "                interaction[question_id] = 1.0\n",
    "                if int(answers[i]) == 1:\n",
    "                    interaction[question_id + questions_counter] = 1.0\n",
    "                    # interaction[-1] = 1.0\n",
    "                \n",
    "                if i < int(lines[lines_counter]) - 1:\n",
    "                    next_question_number = questions[i+1]\n",
    "                    next_question_id = int(next_question_number)\n",
    "                    interaction[-2] = next_question_id\n",
    "                    # Set y\n",
    "                    interaction[-1] = int(answers[i+1])\n",
    "                \n",
    "                interactions.pop(question_id, None)\n",
    "                interactions[question_id] = interaction\n",
    "                \n",
    "            except ValueError:\n",
    "                    pass\n",
    "                        \n",
    "        lines_counter+=3\n",
    "\n",
    "        # Ignore students with 1 interaction\n",
    "        if len(interactions) < 2:\n",
    "            continue\n",
    "\n",
    "        interactions_list = list(interactions.values())\n",
    "\n",
    "        # Padding\n",
    "        for i in range( questions_counter - len(interactions_list) ):\n",
    "            interactions_list.append([0] * (questions_counter * 2 + 2))\n",
    "        \n",
    "        data.append(interactions_list)\n",
    "\n",
    "    if split:\n",
    "        random.shuffle(data)\n",
    "        size = len(data)\n",
    "        split1 = data[: size - size // 80]\n",
    "        split2 = data[size - size // 80:]\n",
    "        return torch.tensor(split1), torch.tensor(split2)\n",
    "\n",
    "    return torch.tensor(data)\n",
    "\n",
    "training_data, validation_data = load_data('assistments2009/train.csv', split=True)\n",
    "\n",
    "training_data = training_data.to(device)\n",
    "validation_data = validation_data.to(device)\n",
    "\n",
    "print('training_data.shape',training_data.shape)\n",
    "print('validation_data.shape',validation_data.shape)\n",
    "\n",
    "training_data = MYDataset(training_data)\n",
    "validation_data = MYDataset(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f2200f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, dropout):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(questions_counter * 2, hidden_size=questions_counter,\n",
    "                        #   batch_first=True\n",
    "                          )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.rnn(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "# model = NeuralNetwork(dropout=0.25)\n",
    "# x = torch.rand(questions_counter, questions_counter * 2)\n",
    "# print(model(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a24884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(params, loss_fn):\n",
    "    print(params)\n",
    "    model = NeuralNetwork(dropout=params['dropout']).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, gamma=params['gamma'], step_size = 1)\n",
    "    epochs = params['epochs']\n",
    "    v_loss = 1_000_000\n",
    "    model_path = ''\n",
    "    no_change_counter = 0\n",
    "    for epoch in range(epochs):\n",
    "        print(f'\\nEpoch {epoch + 1}\\n-------------------------------')\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        model.train()\n",
    "        last_loss = 0\n",
    "        for id, student in enumerate(training_data):\n",
    "            input = student[:, :-2]\n",
    "            truth = student[:, -1]\n",
    "            q_ids = student[:, -2].long()\n",
    "            assert(truth[-1] == 0)\n",
    "            optimizer.zero_grad()\n",
    "            # pred = torch.squeeze(model(input))\n",
    "            pred = model(input)\n",
    "            truth2 = pred.detach().clone()\n",
    "            batch_indices = torch.arange(len(q_ids), device=device)\n",
    "            truth2[batch_indices, q_ids] = truth\n",
    "            loss = loss_fn(pred,truth2)\n",
    "            last_loss = loss.item()\n",
    "            loss.backward()       \n",
    "            optimizer.step()\n",
    "            \n",
    "        # print(f'lr {lr:8f} train loss {last_loss:.8f} ')\n",
    "        \n",
    "        v_losses = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for id, data in enumerate(validation_data):\n",
    "                input = data[:, :-2]\n",
    "                truth = data[:, -1]\n",
    "                q_ids = data[:, -2].long()\n",
    "                assert(truth[-1] == 0)\n",
    "                pred = model(input)\n",
    "                truth2 = pred.detach().clone()\n",
    "                batch_indices = torch.arange(len(q_ids), device=device)\n",
    "                truth2[batch_indices, q_ids] = truth\n",
    "                loss = loss_fn(pred, truth2)\n",
    "                v_losses.append(loss)\n",
    "            \n",
    "        v_loss_epoch = sum(v_losses) / len(v_losses)\n",
    "        print(f'lr {lr:8f} train loss {last_loss:.8f} val loss {v_loss_epoch:.8f}')\n",
    "\n",
    "        if v_loss - v_loss_epoch > 0.00001:\n",
    "            v_loss = v_loss_epoch\n",
    "            no_change_counter = 0\n",
    "            model_path = '-'.join(str(value) for value in params.values()) + 'chkpnt.pt'\n",
    "            torch.save(model.state_dict(), model_path )\n",
    "        elif no_change_counter > params['patience'] - 1:\n",
    "            break\n",
    "        else:\n",
    "            no_change_counter += 1\n",
    "        \n",
    "        scheduler.step()\n",
    "    print('v_loss', v_loss, model_path)\n",
    "\n",
    "# Tune the model\n",
    "# for loss in [nn.BCELoss() ]:\n",
    "#     for lr in [5e-2,1e-2,1e-3,1e-4,1e-5]:\n",
    "#         for gamma in [0.75, 0.9]:\n",
    "#             for dropout in [ 0.1, 0.25]:\n",
    "#                 print(loss)\n",
    "#                 train(params = {'lr':lr, 'gamma': gamma, 'epochs':10, 'patience':3, 'verbose': False, 'dropout' : dropout,  }, loss_fn = loss)\n",
    "\n",
    "train(params = {'lr':1e-5, 'gamma': 0.75, 'epochs':5, 'patience':3, 'verbose': False, 'dropout' : 0.25,  }, loss_fn = nn.BCELoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a4e1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_data = load_data('assistments2009/test.csv',)\n",
    "model = NeuralNetwork(dropout=0.25).to(device)\n",
    "# model.load_state_dict(torch.load(''))\n",
    "\n",
    "y_true = []\n",
    "y_scores = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for id, student in enumerate(testing_data):\n",
    "        input = data[:, :-2]\n",
    "        truth = data[:, -1]\n",
    "        q_ids = data[:, -2].long()\n",
    "        assert(truth[-1] == 0)\n",
    "        pred = model(input)\n",
    "        # Skip students with 1 interaction\n",
    "        last_interaction_id = 0\n",
    "        for j, interaction in enumerate( data):\n",
    "            if torch.equal(interaction, torch.zeros(questions_counter * 3 + 1)):\n",
    "                last_interaction_id = j - 1\n",
    "                break\n",
    "        if last_interaction_id == 0:\n",
    "            continue\n",
    "        \n",
    "        input = data[:, :-1]\n",
    "        truth = data[:, -1]\n",
    "        assert(truth[-1] == 0)\n",
    "        pred = torch.squeeze(model(input))\n",
    "        for i in range(last_interaction_id):\n",
    "            y_true.append(truth[i])\n",
    "            y_scores.append(pred[i])\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "auc = roc_auc_score(y_true, y_scores)\n",
    "print(\"AUC:\", auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
